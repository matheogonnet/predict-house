{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project: Theoretical Formalism\n",
    "\n",
    "This notebook provides a theoretical explanation and mathematical background for the methods used in this project. The focus is on the techniques applied in regression and classification tasks, explaining their assumptions, parameter impacts, and performance evaluation metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### Purpose of EDA\n",
    "Exploratory Data Analysis is the process of summarizing the main characteristics of the dataset, often using visual methods. The goals include:\n",
    "- Identifying data distributions.\n",
    "- Detecting missing values and outliers.\n",
    "- Understanding feature correlations.\n",
    "\n",
    "### Key Techniques Used\n",
    "1. **Descriptive Statistics**: Mean, median, standard deviation, and quartiles.\n",
    "2. **Correlation Analysis**: Pearson correlation coefficient was used to measure linear relationships between features.\n",
    "   $$\n",
    "   r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\n",
    "   $$\n",
    "3. **Visualization Tools**: Histograms, heatmaps, and boxplots provided insights into feature distributions and relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Regression Models\n",
    "\n",
    "### 2.1 Linear Regression\n",
    "\n",
    "**Objective**: Predict a continuous target variable (`MedHouseVal`) using a linear combination of features.\n",
    "\n",
    "**Mathematical Model**:\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n\n",
    "$$\n",
    "where:\n",
    "- $\\hat{y}$ is the predicted value.\n",
    "- $x_i$ are the input features.\n",
    "- $\\beta_i$ are the model coefficients.\n",
    "\n",
    "**Key Assumptions**:\n",
    "1. Linearity: The relationship between features and the target is linear.\n",
    "2. Independence: Observations are independent.\n",
    "3. Homoscedasticity: Residuals (errors) have constant variance.\n",
    "4. Normality: Residuals are normally distributed.\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- **Mean Squared Error (MSE)**:\n",
    "  $$\n",
    "  MSE = \\frac{1}{n}\\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n",
    "  $$\n",
    "- **Mean Absolute Error (MAE)**:\n",
    "  $$\n",
    "  MAE = \\frac{1}{n}\\sum_{i=1}^n |\\hat{y}_i - y_i|\n",
    "  $$\n",
    "- **R² Score**:\n",
    "  $$\n",
    "  R^2 = 1 - \\frac{\\sum (\\hat{y}_i - y_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Random Forest Regression\n",
    "\n",
    "**Objective**: Build an ensemble of decision trees to improve prediction accuracy.\n",
    "\n",
    "**Key Concepts**:\n",
    "1. **Bootstrap Aggregation (Bagging)**: Subsamples are drawn with replacement to train individual trees.\n",
    "2. **Decision Tree Splitting**: Trees are split to minimize variance:\n",
    "   $$\n",
    "   \\text{Variance Reduction} = \\text{Var(Parent)} - \\left(\\text{Weight}_\\text{left} \\cdot \\text{Var(Left)} + \\text{Weight}_\\text{right} \\cdot \\text{Var(Right)}\\right)\n",
    "   $$\n",
    "\n",
    "**Advantages**:\n",
    "- Handles non-linear relationships.\n",
    "- Reduces overfitting by averaging multiple trees.\n",
    "\n",
    "**Feature Importance**: Measures how much each feature contributes to reducing error. Calculated as the total reduction in impurity (e.g., variance) caused by splits involving that feature.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Classification Models\n",
    "\n",
    "### 3.1 K-Nearest Neighbors (KNN)\n",
    "\n",
    "**Objective**: Classify an instance based on the majority class of its $k$-nearest neighbors.\n",
    "\n",
    "**Mathematical Model**:\n",
    "1. **Distance Metric**: Typically uses Euclidean distance:\n",
    "   $$\n",
    "   d(i, j) = \\sqrt{\\sum_{k=1}^n (x_{ik} - x_{jk})^2}\n",
    "   $$\n",
    "2. **Classification Rule**: Assign the class most common among the $k$-nearest neighbors.\n",
    "\n",
    "**Hyperparameter**:\n",
    "- $k$: Number of neighbors. A small $k$ leads to high variance, while a large $k$ leads to high bias.\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- **Accuracy**:\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "  $$\n",
    "- **Confusion Matrix**: Visualizes predictions across classes.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Decision Tree Classifier\n",
    "\n",
    "**Objective**: Build a tree to classify instances by splitting features based on information gain or impurity reduction.\n",
    "\n",
    "**Key Splitting Metrics**:\n",
    "1. **Gini Impurity**:\n",
    "   $$\n",
    "   G = 1 - \\sum_{i=1}^C p_i^2\n",
    "   $$\n",
    "   where $p_i$ is the proportion of class $i$.\n",
    "2. **Entropy**:\n",
    "   $$\n",
    "   H = -\\sum_{i=1}^C p_i \\log_2(p_i)\n",
    "   $$\n",
    "\n",
    "**Advantages**:\n",
    "- Easy to interpret.\n",
    "- Captures non-linear relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Importance in Random Forest\n",
    "\n",
    "Feature importance quantifies the contribution of each feature to the model’s predictive power. In Random Forest, this is computed as:\n",
    "$$\n",
    "\\text{Importance}(f) = \\sum_{t \\in T} \\frac{\\text{Reduction in Impurity from Splits on } f}{\\text{Total Reduction in Impurity}}\n",
    "$$\n",
    "This helps identify key predictors in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "This notebook outlines the theoretical foundations of:\n",
    "1. **Exploratory Data Analysis (EDA)**: Understanding data characteristics.\n",
    "2. **Regression**: Modeling continuous outcomes with Linear Regression and Random Forests.\n",
    "3. **Classification**: Modeling discrete outcomes with KNN and Decision Trees.\n",
    "4. **Evaluation Metrics**: Quantitative measures of model performance.\n",
    "\n",
    "For future work, consider:\n",
    "- Testing additional models (e.g., Support Vector Machines, Gradient Boosting).\n",
    "- Applying advanced feature engineering techniques.\n",
    "- Incorporating cross-validation for robust performance evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
